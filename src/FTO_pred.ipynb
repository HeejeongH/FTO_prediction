{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a51b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caed67cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n",
      "Class 0: 23612개\n",
      "Class 1: 379개\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "Class 0: 23467개\n",
      "Class 1: 524개\n"
     ]
    }
   ],
   "source": [
    "for ratio in ['5x', '10x']:\n",
    "    # 1. 훈련 시와 동일한 방식으로 데이터 준비\n",
    "    selected_descriptor = pd.read_csv('../data/descriptor_selection.csv')\n",
    "    file_md_list = {}\n",
    "    for column in selected_descriptor.columns:\n",
    "        filename = column\n",
    "        selected_columns = selected_descriptor[column].iloc[0:].dropna().tolist()\n",
    "        if filename and selected_columns:\n",
    "            file_md_list[filename] = selected_columns\n",
    "\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "\n",
    "    # 2. 훈련 데이터로 PyCaret 환경 설정\n",
    "    train_data_path = f'../data/preprocessed/filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    filtered_train_df = train_df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    from pycaret.classification import *\n",
    "    exp = setup(\n",
    "        data=filtered_train_df, \n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 3. 모델 로드 (PyCaret 방식)\n",
    "    model = load_model(f'../result/FTO_Final/{ratio}_w3D/blend_models/{ratio}_blended_model2')\n",
    "\n",
    "    # 4. FooDB 데이터 준비 (동일한 컬럼 구조)\n",
    "    foodb_df = pd.read_csv(f\"../data/foodb/filtered_foodb_{ratio}.csv\")\n",
    "    foodb_prediction_data = foodb_df[fp_cols + md_cols].dropna()\n",
    "\n",
    "    # 5. 예측 (자동으로 동일한 정규화 적용)\n",
    "    predictions = predict_model(model, data=foodb_prediction_data, verbose=False, probability_threshold=0.7)\n",
    "\n",
    "    print(f\"Class 0: {sum(predictions['prediction_label'] == 0)}개\")\n",
    "    print(f\"Class 1: {sum(predictions['prediction_label'] == 1)}개\")\n",
    "\n",
    "    # 6. 결과 저장\n",
    "    #predictions.to_csv(f'foodb_predictions_{ratio}_pycaret.csv', index=False)\n",
    "    summary_df = pd.DataFrame({\n",
    "        'compound_id': range(len(predictions)),\n",
    "        'prediction': predictions['prediction_label'],\n",
    "        'probability': predictions['prediction_score'],\n",
    "    })\n",
    "    compound_result = pd.concat([foodb_df, summary_df], axis=1)[['id','canonical_SMILES','prediction','probability']].drop_duplicates('canonical_SMILES')\n",
    "    true = compound_result[compound_result['prediction'] == 1].sort_values(by='probability', ascending=False)\n",
    "    true.to_csv(f'../result/FTO_Final/{ratio}_w3D/blend_models/foodb_predictions_{ratio}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae315db7",
   "metadata": {},
   "source": [
    "### 결과 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f75c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file=\"../data/foodb/foodb_2020_04_07_json/Compound.json\"\n",
    "compounds = []\n",
    "with open(json_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                compound = json.loads(line)\n",
    "                smiles = compound.get('moldb_smiles')\n",
    "                if smiles:\n",
    "                    compounds.append({\n",
    "                        'id': compound.get('public_id'),\n",
    "                        'name': compound.get('name'),\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "foodb_df = pd.DataFrame(compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4796e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 원본 파일 경로\n",
    "file_path = '../result/FTO_Final/foodb_predictions_summary.xlsx'\n",
    "\n",
    "# 데이터 불러오기\n",
    "five_candidates = pd.read_excel(file_path, sheet_name='5x_predictions')\n",
    "ten_candidates = pd.read_excel(file_path, sheet_name='10x_predictions')\n",
    "optnc_candidates = pd.read_excel(file_path, sheet_name='optnc_predictions')\n",
    "\n",
    "# 교집합 기준 컬럼\n",
    "key_cols = [\"id\", \"public_id\", \"name\", \"canonical_SMILES\"]\n",
    "\n",
    "# 교집합 추출\n",
    "five_ten_overlap = pd.merge(five_candidates, ten_candidates, on=key_cols, how='inner', suffixes=('_5x', '_10x'))\n",
    "five_optnc_overlap = pd.merge(five_candidates, optnc_candidates, on=key_cols, how='inner', suffixes=('_5x', '_optnc'))\n",
    "ten_optnc_overlap = pd.merge(ten_candidates, optnc_candidates, on=key_cols, how='inner', suffixes=('_10x', '_optnc'))\n",
    "\n",
    "# 세개 모두 겹치는 것\n",
    "three_way_overlap = pd.merge(five_candidates, ten_candidates, on=key_cols, how='inner')\n",
    "three_way_overlap = pd.merge(three_way_overlap, optnc_candidates, on=key_cols, how='inner', suffixes=('', '_optnc'))\n",
    "\n",
    "# 기존 파일에 새로운 시트 추가\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    five_ten_overlap.to_excel(writer, sheet_name='5x_10x_overlap', index=False)\n",
    "    five_optnc_overlap.to_excel(writer, sheet_name='5x_optnc_overlap', index=False)\n",
    "    ten_optnc_overlap.to_excel(writer, sheet_name='10x_optnc_overlap', index=False)\n",
    "    three_way_overlap.to_excel(writer, sheet_name='common_predictions', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689e6f9",
   "metadata": {},
   "source": [
    "### 식품 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f9d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file=\"../data/foodb/foodb_2020_04_07_json/Content.json\"\n",
    "contents = []\n",
    "with open(json_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                content = json.loads(line)\n",
    "                food_common_name = content.get('orig_food_common_name')\n",
    "                food_scientific_name = content.get('orig_food_scientific_name')\n",
    "                orig_food_part = content.get('orig_food_part')\n",
    "                standard_content = content.get('standard_content')\n",
    "                orig_unit = content.get('orig_unit')\n",
    "                citation = content.get('citation')\n",
    "\n",
    "                contents.append({\n",
    "                    'source_id': content.get('source_id'),\n",
    "                    'food_id': content.get('food_id'),\n",
    "                    'name': food_common_name,\n",
    "                    'scientific_name': food_scientific_name,\n",
    "                    'part': orig_food_part,\n",
    "                    'content': standard_content,\n",
    "                    'unit': orig_unit,\n",
    "                    'citation': citation\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "content_df = pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20c96e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optnc_food = optnc_candidates.merge(content_df, left_on='id', right_on='source_id')\n",
    "optnc_food = optnc_food.rename(columns={'name_x':'name', 'name_y':'food_common_name', 'content':'standard_content', 'unit':'orig_unit'}).drop(columns=['source_id'])\n",
    "optnc_food = optnc_food[['id', 'public_id', 'name', 'canonical_SMILES', 'probability', 'food_id', 'food_common_name', 'scientific_name', 'part','standard_content', 'orig_unit', 'citation']]\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    optnc_food.to_excel(writer, sheet_name='optnc_foods', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c78f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
