{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e411faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7ce6d",
   "metadata": {},
   "source": [
    "# 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b95986",
   "metadata": {},
   "source": [
    "## 사용할 descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04a3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_descriptor = pd.read_csv('../data/descriptor_selection.csv')\n",
    "\n",
    "file_md_list = {}\n",
    "for column in selected_descriptor.columns:\n",
    "    filename = column\n",
    "    selected_columns = selected_descriptor[column].iloc[0:].dropna().tolist()\n",
    "    if filename and selected_columns:\n",
    "        file_md_list[filename] = selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a81072",
   "metadata": {},
   "source": [
    "## 저장 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75bd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_move_file(src, dst):\n",
    "    if os.path.exists(src):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"파일 이동 실패: {src} -> {dst}, 에러: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def safe_save_csv(df, path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 저장 실패: {path}, 에러: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1521974",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_filenames = {\n",
    "    'auc': 'AUC.png',\n",
    "    'confusion_matrix': 'Confusion Matrix.png',\n",
    "    'learning': 'Learning Curve.png',\n",
    "    'feature': 'Feature Importance.png',\n",
    "    'error': 'Prediction Error.png',\n",
    "    'calibration': 'Calibration Curve.png'\n",
    "}\n",
    "\n",
    "target_models = ['lr', 'et', 'gbc', 'lightgbm', 'svm', 'rf', 'ada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba61c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\", \"preprocessed\")\n",
    "result_dir = os.path.join(\"..\", \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bf1e1",
   "metadata": {},
   "source": [
    "# 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1475b",
   "metadata": {},
   "source": [
    "## 개별 모델 학습 및 블랜딩할 모델 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c3b061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for ratio in [\\'5x\\', \\'10x\\']:\\n    file_name = f\\'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv\\'\\n    base_path = f\\'FTO_Final/{ratio}_w3D\\'\\n    \\n    # 데이터 파일 경로\\n    data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\\n    df = pd.read_csv(data_path)\\n    print(f\"{\\'=\\'*50}{base_path} start{\\'=\\'*50}\")\\n\\n    # 저장 폴더 생성\\n    full_result_path = os.path.join(result_dir, base_path)\\n    models_dir = os.path.join(full_result_path, \"models\")\\n    plots_dir = os.path.join(full_result_path, \"plots\")\\n    \\n    for dir_path in [full_result_path, models_dir, plots_dir]:\\n        os.makedirs(dir_path, exist_ok=True)\\n\\n    md_cols = file_md_list[file_name]\\n    fp_cols = [f\\'X{i+1}\\' for i in range(1024)]\\n    filtered_df = df[[\\'potency\\'] + fp_cols + md_cols]\\n\\n    X = filtered_df.drop(\\'potency\\', axis=1)\\n    Y = filtered_df[\\'potency\\']\\n\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\\n    df_train = pd.concat([x_train, y_train], axis=1)\\n\\n    setup(\\n        data=df_train, \\n        target=\\'potency\\',\\n        session_id=42,\\n        train_size=0.9,\\n        fold=10,\\n        normalize=True,\\n        fix_imbalance=True,\\n        remove_outliers=True,\\n        n_jobs=-1, \\n        verbose=False\\n    )\\n    \\n    summary_data = []\\n    for model_id in target_models:\\n        model = create_model(model_id, verbose=False)\\n        print(f\"Tuning {model_id}...\")\\n        \\n        tuned_model = tune_model(\\n            model, \\n            optimize=\\'F1\\',\\n            n_iter=50,\\n            fold=5,\\n            choose_better=True,\\n            verbose=False\\n        )\\n        results = pull()\\n        model_name = tuned_model.__class__.__name__\\n        \\n        # 모델 저장\\n        model_path = os.path.join(models_dir, f\"{ratio}_{model_name}_model.pkl\")\\n        joblib.dump(tuned_model, model_path)\\n        print(f\"모델 저장 완료\")\\n        \\n        # 평가 결과 저장\\n        eval_path = os.path.join(full_result_path, f\"{ratio}_{model_name}_evaluation.csv\")\\n        safe_save_csv(results, eval_path)\\n        print(f\"평가 결과 저장 완료\")\\n        \\n        # 플롯 생성 및 저장\\n        for plot_type, default_name in default_filenames.items():\\n            try:\\n                print(f\"{plot_type} // {default_name}\")\\n                plot_model(tuned_model, plot=plot_type, save=True, verbose=False)\\n                \\n                if os.path.exists(default_name):\\n                    final_filename = f\"{ratio}_{model_name}_{plot_type}.png\"\\n                    final_save_path = os.path.join(plots_dir, final_filename)\\n                    shutil.move(default_name, final_save_path)\\n                    print(f\"{default_name} 저장 완료\")\\n                else:\\n                    print(f\"{default_name} 파일이 생성되지 않았습니다.\")\\n                    \\n            except Exception as e:\\n                print(f\"[{plot_type}] Plot 생성 실패: {e}\")\\n                \\n                error_filename = f\"{ratio}_{model_name}_{plot_type}_error.txt\"\\n                error_path = os.path.join(plots_dir, error_filename)\\n                with open(error_path, \\'w\\', encoding=\\'utf-8\\') as f:\\n                    f.write(f\"Plot Type: {plot_type}\\n\")\\n                    f.write(f\"Model: {model_name}\\n\")\\n                    f.write(f\"Error: {str(e)}\\n\")\\n                print(f\"에러 정보가 {error_filename}에 저장되었습니다.\")\\n        \\n        numeric_cols = results.select_dtypes(include=[np.number]).columns\\n        avg_row = results[numeric_cols].mean().to_dict()\\n        std_row = results[numeric_cols].std().to_dict()\\n        \\n        avg_row.update({\\'Model\\': model_name, \\'Type\\': \\'Mean\\'})\\n        std_row.update({\\'Model\\': model_name, \\'Type\\': \\'Std\\'})\\n        \\n        summary_data.extend([avg_row, std_row])\\n\\n    # 전체 요약 저장\\n    if summary_data:\\n        combined_summary = pd.DataFrame(summary_data)\\n        summary_path = os.path.join(full_result_path, f\"{ratio}_summary_evaluation.csv\")\\n        safe_save_csv(combined_summary, summary_path)\\n\\n    print(f\"{\\'=\\'*50}{base_path} completed{\\'=\\'*50}\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for ratio in ['5x', '10x']:\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    base_path = f'FTO_Final/{ratio}_w3D'\n",
    "    \n",
    "    # 데이터 파일 경로\n",
    "    data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"{'='*50}{base_path} start{'='*50}\")\n",
    "\n",
    "    # 저장 폴더 생성\n",
    "    full_result_path = os.path.join(result_dir, base_path)\n",
    "    models_dir = os.path.join(full_result_path, \"models\")\n",
    "    plots_dir = os.path.join(full_result_path, \"plots\")\n",
    "    \n",
    "    for dir_path in [full_result_path, models_dir, plots_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "    filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    X = filtered_df.drop('potency', axis=1)\n",
    "    Y = filtered_df['potency']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "    setup(\n",
    "        data=df_train, \n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        n_jobs=-1, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    summary_data = []\n",
    "    for model_id in target_models:\n",
    "        model = create_model(model_id, verbose=False)\n",
    "        print(f\"Tuning {model_id}...\")\n",
    "        \n",
    "        tuned_model = tune_model(\n",
    "            model, \n",
    "            optimize='F1',\n",
    "            n_iter=50,\n",
    "            fold=5,\n",
    "            choose_better=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        results = pull()\n",
    "        model_name = tuned_model.__class__.__name__\n",
    "        \n",
    "        # 모델 저장\n",
    "        model_path = os.path.join(models_dir, f\"{ratio}_{model_name}_model.pkl\")\n",
    "        joblib.dump(tuned_model, model_path)\n",
    "        print(f\"모델 저장 완료\")\n",
    "        \n",
    "        # 평가 결과 저장\n",
    "        eval_path = os.path.join(full_result_path, f\"{ratio}_{model_name}_evaluation.csv\")\n",
    "        safe_save_csv(results, eval_path)\n",
    "        print(f\"평가 결과 저장 완료\")\n",
    "        \n",
    "        # 플롯 생성 및 저장\n",
    "        for plot_type, default_name in default_filenames.items():\n",
    "            try:\n",
    "                print(f\"{plot_type} // {default_name}\")\n",
    "                plot_model(tuned_model, plot=plot_type, save=True, verbose=False)\n",
    "                \n",
    "                if os.path.exists(default_name):\n",
    "                    final_filename = f\"{ratio}_{model_name}_{plot_type}.png\"\n",
    "                    final_save_path = os.path.join(plots_dir, final_filename)\n",
    "                    shutil.move(default_name, final_save_path)\n",
    "                    print(f\"{default_name} 저장 완료\")\n",
    "                else:\n",
    "                    print(f\"{default_name} 파일이 생성되지 않았습니다.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[{plot_type}] Plot 생성 실패: {e}\")\n",
    "                \n",
    "                error_filename = f\"{ratio}_{model_name}_{plot_type}_error.txt\"\n",
    "                error_path = os.path.join(plots_dir, error_filename)\n",
    "                with open(error_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Plot Type: {plot_type}\\n\")\n",
    "                    f.write(f\"Model: {model_name}\\n\")\n",
    "                    f.write(f\"Error: {str(e)}\\n\")\n",
    "                print(f\"에러 정보가 {error_filename}에 저장되었습니다.\")\n",
    "        \n",
    "        numeric_cols = results.select_dtypes(include=[np.number]).columns\n",
    "        avg_row = results[numeric_cols].mean().to_dict()\n",
    "        std_row = results[numeric_cols].std().to_dict()\n",
    "        \n",
    "        avg_row.update({'Model': model_name, 'Type': 'Mean'})\n",
    "        std_row.update({'Model': model_name, 'Type': 'Std'})\n",
    "        \n",
    "        summary_data.extend([avg_row, std_row])\n",
    "\n",
    "    # 전체 요약 저장\n",
    "    if summary_data:\n",
    "        combined_summary = pd.DataFrame(summary_data)\n",
    "        summary_path = os.path.join(full_result_path, f\"{ratio}_summary_evaluation.csv\")\n",
    "        safe_save_csv(combined_summary, summary_path)\n",
    "\n",
    "    print(f\"{'='*50}{base_path} completed{'='*50}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59abae",
   "metadata": {},
   "source": [
    "## 블랜딩 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba9f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================5x Blending Start==============================\n",
      "Creating fresh et model...\n",
      "et tuning completed\n",
      "Creating fresh gbc model...\n",
      "gbc tuning completed\n",
      "Creating fresh lightgbm model...\n",
      "lightgbm tuning completed\n",
      "Creating fresh lr model...\n",
      "lr tuning completed\n",
      "Attempting to create blend model...\n",
      "블렌드 모델 저장 완료: ..\\result\\FTO_Final/5x_w3D\\blend_models\\5x_blended_model2.pkl\n",
      "최종 예측 완료\n",
      "Generating auc plot...\n",
      "auc 플롯 저장 완료\n",
      "Generating confusion_matrix plot...\n",
      "confusion_matrix 플롯 저장 완료\n",
      "Generating learning plot...\n",
      "learning 플롯 저장 완료\n",
      "Generating feature plot...\n",
      "[feature] Plot 생성 실패: Feature Importance and RFE plots not available for estimators that doesnt support coef_ or feature_importances_ attribute.\n",
      "블렌딩 성공: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "==============================5x Blending Complete==============================\n",
      "==============================10x Blending Start==============================\n",
      "Creating fresh et model...\n",
      "et tuning completed\n",
      "Creating fresh gbc model...\n",
      "gbc tuning completed\n",
      "Creating fresh lightgbm model...\n",
      "lightgbm tuning completed\n",
      "Creating fresh lr model...\n",
      "lr tuning completed\n",
      "Attempting to create blend model...\n",
      "블렌드 모델 저장 완료: ..\\result\\FTO_Final/10x_w3D\\blend_models\\10x_blended_model2.pkl\n",
      "최종 예측 완료\n",
      "Generating auc plot...\n",
      "auc 플롯 저장 완료\n",
      "Generating confusion_matrix plot...\n",
      "confusion_matrix 플롯 저장 완료\n",
      "Generating learning plot...\n",
      "learning 플롯 저장 완료\n",
      "Generating feature plot...\n",
      "[feature] Plot 생성 실패: Feature Importance and RFE plots not available for estimators that doesnt support coef_ or feature_importances_ attribute.\n",
      "블렌딩 성공: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "==============================10x Blending Complete==============================\n",
      "\n",
      "블렌딩 결과 요약:\n",
      "\n",
      "[5x] Blend Model:\n",
      "  - 사용된 모델: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "  - 블렌드 모델 평균 F1: 0.6836\n",
      "  - 개별 모델 F1 점수:\n",
      "    et: 0.7303\n",
      "    gbc: 0.7353\n",
      "    lightgbm: 0.7186\n",
      "    lr: 0.7320\n",
      "\n",
      "[10x] Blend Model:\n",
      "  - 사용된 모델: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "  - 블렌드 모델 평균 F1: 0.6613\n",
      "  - 개별 모델 F1 점수:\n",
      "    et: 0.7102\n",
      "    gbc: 0.7195\n",
      "    lightgbm: 0.7125\n",
      "    lr: 0.7299\n",
      "\n",
      "모든 블렌딩 작업이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "blend_models_list = ['et', 'gbc', 'lightgbm', 'lr']\n",
    "\n",
    "all_blended_results = {}\n",
    "\n",
    "for ratio in ['5x', '10x']:\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    base_path = f'FTO_Final/{ratio}_w3D'\n",
    "    \n",
    "    data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"{'='*30}{ratio} Blending Start{'='*30}\")\n",
    "\n",
    "    full_result_path = os.path.join(result_dir, base_path)\n",
    "    blend_models_dir = os.path.join(full_result_path, \"blend_models\")\n",
    "    blend_plots_dir = os.path.join(full_result_path, \"blend_plots\")\n",
    "    \n",
    "    for dir_path in [blend_models_dir, blend_plots_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "    filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    X = filtered_df.drop('potency', axis=1)\n",
    "    Y = filtered_df['potency']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "    exp = setup(\n",
    "        data=df_train, \n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        n_jobs=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    fresh_models = []\n",
    "    model_names = []\n",
    "    individual_results = {}\n",
    "    \n",
    "    for model_id in blend_models_list:\n",
    "        print(f\"Creating fresh {model_id} model...\")\n",
    "        if model_id == 'lightgbm':\n",
    "            lgb_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.1,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'min_child_samples': 20,\n",
    "                'random_state': 42,\n",
    "                'n_estimators': 100,\n",
    "                'verbosity': -1\n",
    "            }\n",
    "            fresh_model = create_model(model_id, verbose=False, **lgb_params)\n",
    "        else:\n",
    "            fresh_model = create_model(model_id, verbose=False)\n",
    "            \n",
    "        tuned_model = tune_model(\n",
    "            fresh_model, \n",
    "            optimize='F1',\n",
    "            n_iter=50,\n",
    "            fold=5,\n",
    "            choose_better=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        model_results = pull()\n",
    "        individual_results[model_id] = model_results\n",
    "        \n",
    "        fresh_models.append(tuned_model)\n",
    "        model_names.append(tuned_model.__class__.__name__)\n",
    "        \n",
    "        print(f\"{model_id} tuning completed\")        \n",
    "    \n",
    "    print(\"Attempting to create blend model...\")\n",
    "    blend_success = False\n",
    "    \n",
    "    blend_attempts = [\n",
    "            ]\n",
    "    \n",
    "    blended_model = blend_models(\n",
    "        estimator_list=fresh_models,\n",
    "        verbose=False,\n",
    "        **{'fold': 3, 'method': 'soft'},\n",
    "    )\n",
    "    blend_results = pull()\n",
    "    blend_success = True\n",
    "\n",
    "    blend_model_path = os.path.join(blend_models_dir, f\"{ratio}_blended_model2.pkl\")\n",
    "    joblib.dump(blended_model, blend_model_path)\n",
    "    print(f\"블렌드 모델 저장 완료: {blend_model_path}\")\n",
    "    \n",
    "    blend_eval_path = os.path.join(full_result_path, f\"{ratio}_blend_evaluation.csv\")\n",
    "    safe_save_csv(blend_results, blend_eval_path)\n",
    "    \n",
    "    # 테스트 데이터 예측\n",
    "    test_data = pd.concat([x_test, y_test], axis=1)\n",
    "    try:\n",
    "        final_predictions = predict_model(blended_model, data=test_data, verbose=False)\n",
    "        final_metrics = pull()\n",
    "        final_metrics_path = os.path.join(full_result_path, f\"{ratio}_blend_final_metrics.csv\")\n",
    "        safe_save_csv(final_metrics, final_metrics_path)\n",
    "        print(\"최종 예측 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"Final prediction failed: {e}\")\n",
    "        final_metrics = None\n",
    "    \n",
    "    # 플롯 생성\n",
    "    plot_types = ['auc', 'confusion_matrix', 'learning', 'feature']\n",
    "    for plot_type in plot_types:\n",
    "        try:\n",
    "            print(f\"Generating {plot_type} plot...\")\n",
    "            plot_model(blended_model, plot=plot_type, save=True, verbose=False)\n",
    "            \n",
    "            default_name = default_filenames.get(plot_type, f'{plot_type}.png')\n",
    "            if os.path.exists(default_name):\n",
    "                final_filename = f\"{ratio}_blend_{plot_type}.png\"\n",
    "                final_save_path = os.path.join(blend_plots_dir, final_filename)\n",
    "                safe_move_file(default_name, final_save_path)\n",
    "                print(f\"{plot_type} 플롯 저장 완료\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[{plot_type}] Plot 생성 실패: {e}\")\n",
    "            error_filename = f\"{ratio}_blend_{plot_type}_error.txt\"\n",
    "            error_path = os.path.join(blend_plots_dir, error_filename)\n",
    "            with open(error_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Plot Type: {plot_type}\\nModel: Blended\\nError: {str(e)}\\n\")\n",
    "    \n",
    "    # 개별 모델 결과 저장\n",
    "    for model_id, results in individual_results.items():\n",
    "        individual_path = os.path.join(full_result_path, f\"{ratio}_{model_id}_individual_results.csv\")\n",
    "        safe_save_csv(results, individual_path)\n",
    "    \n",
    "    all_blended_results[ratio] = {\n",
    "        'blend_results': blend_results,\n",
    "        'final_metrics': final_metrics,\n",
    "        'model_names': model_names,\n",
    "        'individual_results': individual_results\n",
    "    }\n",
    "    \n",
    "    print(f\"블렌딩 성공: {', '.join(model_names)}\")    \n",
    "    print(f\"{'='*30}{ratio} Blending Complete{'='*30}\")\n",
    "\n",
    "print(\"\\n블렌딩 결과 요약:\")\n",
    "for ratio, results in all_blended_results.items():\n",
    "    print(f\"\\n[{ratio}] Blend Model:\")\n",
    "    print(f\"  - 사용된 모델: {', '.join(results['model_names'])}\")\n",
    "    if 'blend_results' in results and results['blend_results'] is not None:\n",
    "        if 'F1' in results['blend_results'].columns:\n",
    "            blend_f1 = results['blend_results']['F1'].mean()\n",
    "            print(f\"  - 블렌드 모델 평균 F1: {blend_f1:.4f}\")\n",
    "    print(\"  - 개별 모델 F1 점수:\")\n",
    "    for model_id, result_df in results['individual_results'].items():\n",
    "        if 'F1' in result_df.columns:\n",
    "            individual_f1 = result_df['F1'].mean()\n",
    "            print(f\"    {model_id}: {individual_f1:.4f}\")\n",
    "\n",
    "print(\"\\n모든 블렌딩 작업이 완료되었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
